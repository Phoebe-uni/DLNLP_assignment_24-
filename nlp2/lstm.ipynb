{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8fc7c2e9-d7fa-4ebf-961d-09e93da8f5ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\work\\ffwork\\nlp\\nlp1\\input\\sample_submission.csv\n",
      "D:\\work\\ffwork\\nlp\\nlp1\\input\\test.csv\n",
      "D:\\work\\ffwork\\nlp\\nlp1\\input\\train.csv\n",
      "D:\\work\\ffwork\\nlp\\nlp1\\input\\twitter_training.csv\n",
      "D:\\work\\ffwork\\nlp\\nlp1\\input\\twitter_training.csv.zip\n",
      "D:\\work\\ffwork\\nlp\\nlp1\\input\\twitter_validation.csv\n",
      "D:\\work\\ffwork\\nlp\\nlp1\\input\\vocab.txt\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "dir_path = Path().cwd()\n",
    "inp_path = dir_path /'input'\n",
    "for dirname, _, filenames in os.walk(inp_path):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "87cb6e03-ad12-40d3-b0ce-ccef5e54e50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from nltk.corpus import stopwords \n",
    "from collections import Counter\n",
    "import string\n",
    "import re\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "512f918a-8c87-490f-b462-3c82895756ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/train.csv')\n",
    "# test_df = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/test.csv')\n",
    "# train_df.head()\n",
    "train_df = pd.read_csv('./input/twitter_training.csv', header=None)\n",
    "val_df = pd.read_csv('./input/twitter_validation.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "de0f3089-67a3-439c-aa3e-d610d30bf87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.reset_index(drop=True,inplace=True)\n",
    "val_df.reset_index(drop=True,inplace=True)\n",
    "df = pd.concat([train_df,val_df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1a07b9cc-3fdb-4151-ae5d-4e76bd801e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop([0], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e9fc260c-a167-44a7-a127-9cf15fbbe24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['platform','sentiment','text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7b2c1957-7f6c-436a-b644-d8f2834d5f17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>platform</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will murder yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>I am coming to the borders and I will kill you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will kill you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im coming on borderlands and i will murder you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands 2 and i will murder ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>GrandTheftAuto(GTA)</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>⭐️ Toronto is the arts and culture capital of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>CS-GO</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>tHIS IS ACTUALLY A GOOD MOVE TOT BRING MORE VI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Today sucked so it’s time to drink wine n play...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>Microsoft</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Bought a fraction of Microsoft today. Small wins.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>johnson&amp;johnson</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Johnson &amp; Johnson to stop selling talc baby po...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>75682 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                platform   sentiment  \\\n",
       "0            Borderlands    Positive   \n",
       "1            Borderlands    Positive   \n",
       "2            Borderlands    Positive   \n",
       "3            Borderlands    Positive   \n",
       "4            Borderlands    Positive   \n",
       "..                   ...         ...   \n",
       "995  GrandTheftAuto(GTA)  Irrelevant   \n",
       "996                CS-GO  Irrelevant   \n",
       "997          Borderlands    Positive   \n",
       "998            Microsoft    Positive   \n",
       "999      johnson&johnson     Neutral   \n",
       "\n",
       "                                                  text  \n",
       "0    im getting on borderlands and i will murder yo...  \n",
       "1    I am coming to the borders and I will kill you...  \n",
       "2    im getting on borderlands and i will kill you ...  \n",
       "3    im coming on borderlands and i will murder you...  \n",
       "4    im getting on borderlands 2 and i will murder ...  \n",
       "..                                                 ...  \n",
       "995  ⭐️ Toronto is the arts and culture capital of ...  \n",
       "996  tHIS IS ACTUALLY A GOOD MOVE TOT BRING MORE VI...  \n",
       "997  Today sucked so it’s time to drink wine n play...  \n",
       "998  Bought a fraction of Microsoft today. Small wins.  \n",
       "999  Johnson & Johnson to stop selling talc baby po...  \n",
       "\n",
       "[75682 rows x 3 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0548cebe-057c-4e18-ab3a-38cf26a1a01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['platform'], axis=1, inplace=True)\n",
    "df.sentiment = df.sentiment.map({\"Neutral\":0, \"Irrelevant\":0 ,\"Positive\":1,\"Negative\":2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d619f40c-cac4-4d16-a7ff-07cdd195d6e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment      0\n",
       "text         686\n",
       "dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2f26ded5-40ac-452d-8df6-c23ccb51c919",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966a0139-b7d7-4a1f-a654-996c3e23c117",
   "metadata": {},
   "source": [
    "Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cb629d36-2351-44dc-a3ad-6793dbf2fb06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\南京中略信息技术有限公司\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import nltk\n",
    "#nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9689a1a-bc71-474c-b2e1-bc82709db1fe",
   "metadata": {},
   "source": [
    "Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eeab0724-c192-461a-b958-cf653b62deb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>im getting borderlands murder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>coming borders kill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>im getting borderlands kill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>im coming borderlands murder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>im getting borderlands 2 murder</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                             text\n",
       "0          1    im getting borderlands murder\n",
       "1          1              coming borders kill\n",
       "2          1      im getting borderlands kill\n",
       "3          1     im coming borderlands murder\n",
       "4          1  im getting borderlands 2 murder"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "def data_preprocessing(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub('<.*?>', '', text) # Remove HTML from text\n",
    "    text = ''.join([c for c in text if c not in string.punctuation])# Remove punctuation\n",
    "    text = [word for word in text.split() if word not in stop_words]\n",
    "    text = ' '.join(text)\n",
    "    return text\n",
    "\n",
    "df['text'] = df['text'].astype(str).apply(data_preprocessing)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fddbcb0-cf6b-41e3-a113-f8e3c1280788",
   "metadata": {},
   "source": [
    "Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7239bc27-4306-4ef3-ae77-e0f86ca3b99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [word for text in df['text'] for word in text.split()]\n",
    "count_words = Counter(corpus)\n",
    "sorted_words = count_words.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "36826489-0003-40dd-bebc-75961691289c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_to_int = {w:i+1 for i, (w,c) in enumerate(sorted_words)}\n",
    "\n",
    "text_int = []\n",
    "for text in df['text']:\n",
    "    r = [vocab_to_int[word] for word in text.split()]\n",
    "    text_int.append(r)\n",
    "\n",
    "df['text int'] = text_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bfffce1d-d7ce-41cf-9761-1d1649dbf925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163\n"
     ]
    }
   ],
   "source": [
    "text_len = [len(x) for x in text_int]\n",
    "max_len = max(text_len)\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc4e60b-aceb-4ae5-a075-aee6f365ebe3",
   "metadata": {},
   "source": [
    "Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de368d7d-6cfd-4412-8618-c7c74d4ed207",
   "metadata": {},
   "source": [
    "Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "56ada56b-8815-46b4-82d7-c0d315e730ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Padding(text_int, seq_len):\n",
    "    '''\n",
    "    Return features of text_ints, where each text is padded with 0's\n",
    "    '''\n",
    "    features = np.zeros((len(text_int), seq_len), dtype = int)\n",
    "    for i, text in enumerate(text_int):\n",
    "        if len(text) <= seq_len:\n",
    "            zeros = list(np.zeros(seq_len - len(text)))\n",
    "            new = zeros + text\n",
    "        features[i, :] = np.array(new)\n",
    "            \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a22b6df5-3694-47c8-8722-169f4180625a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    5   83   57 1610]\n"
     ]
    }
   ],
   "source": [
    "features = Padding(text_int, max_len)\n",
    "\n",
    "print(features[0, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26868f56-bcf2-429e-9474-f3cc2fc331f3",
   "metadata": {},
   "source": [
    "Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2014dbbc-4395-4ed5-bc9b-82915ea60a7f",
   "metadata": {},
   "source": [
    "Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b5e2f87e-a6df-41bc-b5e4-bf8cccd8d303",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_remain, y_train, y_remain = train_test_split(features, df['sentiment'].to_numpy(), test_size=0.2, random_state=1)\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_remain, y_remain, test_size=0.5, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c261c7d9-e7e2-41dd-99c8-0b96e67f0c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tensor dataset\n",
    "train_data = TensorDataset(torch.from_numpy(X_train.astype(float)), torch.from_numpy(y_train.astype(float)))\n",
    "test_data = TensorDataset(torch.from_numpy(X_test.astype(float)), torch.from_numpy(y_test.astype(float)))\n",
    "valid_data = TensorDataset(torch.from_numpy(X_valid.astype(float)), torch.from_numpy(y_valid.astype(float)))\n",
    "\n",
    "# dataloaders\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7500ea67-a716-4098-80eb-7c820b207e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input size:  torch.Size([32, 163])\n",
      "Sample output size:  torch.Size([32])\n",
      "Sample input: \n",
      " tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.1960e+03, 5.3200e+02,\n",
      "         9.9440e+03],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.8210e+03, 1.9980e+03,\n",
      "         3.7773e+04],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.9000e+01, 1.2000e+03,\n",
      "         7.3700e+02],\n",
      "        ...,\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 1.0800e+02,\n",
      "         5.8000e+02],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 9.7590e+03, 1.3900e+02,\n",
      "         3.6220e+04],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.3100e+02, 2.6000e+01,\n",
      "         2.1400e+03]], dtype=torch.float64)\n",
      "Sample input: \n",
      " tensor([2., 0., 0., 1., 1., 2., 0., 0., 2., 0., 0., 0., 0., 0., 2., 2., 1., 0.,\n",
      "        2., 0., 0., 1., 2., 0., 2., 1., 2., 0., 0., 0., 0., 2.],\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(train_loader)\n",
    "sample_x, sample_y = next(dataiter)\n",
    "\n",
    "print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n",
    "print('Sample output size: ', sample_y.size())\n",
    "print('Sample input: \\n', sample_x)\n",
    "print('Sample input: \\n', sample_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe049906-8b51-4255-8805-82e2473a8ac9",
   "metadata": {},
   "source": [
    "Define model class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14a82fd-ac27-4eff-b81c-006d272d5b87",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f5bffda1-a5eb-455a-b046-0325d65e2be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class sentimentLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    The RNN model that will be used to perform Sentiment analysis.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers):\n",
    "        \"\"\"\n",
    "        Initialize the model by setting up the layers.\n",
    "        \"\"\"\n",
    "        super(sentimentLSTM, self).__init__()\n",
    "        \n",
    "        self.output_size = output_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # Embedding and LSTM layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, batch_first=True)\n",
    "        \n",
    "\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        \n",
    "\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "\n",
    "        embeds = self.embedding(x.int())\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "        lstm_out = lstm_out[:, -1]\n",
    "        out = self.fc(lstm_out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        h0 = torch.zeros((self.n_layers,batch_size,self.hidden_dim)).to(device)\n",
    "        c0 = torch.zeros((self.n_layers,batch_size,self.hidden_dim)).to(device)\n",
    "        hidden = (h0,c0)\n",
    "        return hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "eedcecd2-e825-4539-8ecb-e34ea2f50109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n",
      "sentimentLSTM(\n",
      "  (embedding): Embedding(42037, 64)\n",
      "  (lstm): LSTM(64, 256, batch_first=True)\n",
      "  (fc): Linear(in_features=256, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")\n",
    "\n",
    "# Instantiate the model w/ hyperparams\n",
    "vocab_size = len(vocab_to_int) + 1\n",
    "output_size = 3\n",
    "embedding_dim = 64\n",
    "hidden_dim = 256\n",
    "n_layers = 1\n",
    "\n",
    "model = sentimentLSTM(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
    "model = model.to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03242952-df92-453e-bc83-11c530be7318",
   "metadata": {},
   "source": [
    "Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e918a354-1e49-4e40-b5de-58cb6c594298",
   "metadata": {},
   "source": [
    "Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d9aab483-fbb0-4fe7-88ed-f8c8fab1216f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() # multi-class, if binary, then use BCELoss()\n",
    "\n",
    "def acc(pred,label):\n",
    "    pred = pred.argmax(1)\n",
    "    return torch.sum(pred == label.squeeze()).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3116ccc2-f025-46d5-986b-4ded0e118087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "train_loss : 0.8693432961704891 val_loss : 0.7009588476174917\n",
      "train_accuracy : 58.97726515101007 val_accuracy : 69.49333333333333\n",
      "Validation loss decreased (inf --> 0.700959).  Saving model ...\n",
      "==================================================\n",
      "Epoch 2\n",
      "train_loss : 0.49242197117880543 val_loss : 0.4571829812012167\n",
      "train_accuracy : 80.2370158010534 val_accuracy : 82.33333333333334\n",
      "Validation loss decreased (0.700959 --> 0.457183).  Saving model ...\n",
      "==================================================\n",
      "Epoch 3\n",
      "train_loss : 0.21958540033115712 val_loss : 0.4024176432026757\n",
      "train_accuracy : 91.78111874124941 val_accuracy : 85.78666666666666\n",
      "Validation loss decreased (0.457183 --> 0.402418).  Saving model ...\n",
      "==================================================\n",
      "Epoch 4\n",
      "train_loss : 0.110486141282751 val_loss : 0.4201985192286153\n",
      "train_accuracy : 95.73638242549504 val_accuracy : 86.06666666666666\n",
      "==================================================\n",
      "Epoch 5\n",
      "train_loss : 0.08215279881323101 val_loss : 0.45022211465825385\n",
      "train_accuracy : 96.70144676311753 val_accuracy : 86.61333333333333\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "clip = 5\n",
    "epochs = 5 \n",
    "valid_loss_min = np.Inf\n",
    "# train for some number of epochs\n",
    "epoch_tr_loss,epoch_vl_loss = [],[]\n",
    "epoch_tr_acc,epoch_vl_acc = [],[]\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_losses = []\n",
    "    train_acc = 0.0\n",
    "    model.train()\n",
    "    correct = 0\n",
    "    # initialize hidden state \n",
    "    h = model.init_hidden(batch_size)\n",
    "    for inputs, labels in train_loader:\n",
    "        \n",
    "        inputs, labels = inputs.to(device), labels.to(device)   \n",
    "        # Creating new variables for the hidden state, otherwise\n",
    "        # we'd backprop through the entire training history\n",
    "        h = tuple([each.data for each in h])\n",
    "        model.zero_grad()\n",
    "        output = model(inputs,h)\n",
    "        # calculate the loss and perform backprop\n",
    "        loss = criterion(output, labels.long())\n",
    "        loss.backward()\n",
    "        train_losses.append(loss.item())\n",
    "        # accuracy\n",
    "        accuracy = acc(output,labels)\n",
    "        train_acc += accuracy\n",
    "        #`clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "    val_h = model.init_hidden(batch_size)\n",
    "    val_losses = []\n",
    "    val_acc = 0.0\n",
    "    model.eval()\n",
    "    for inputs, labels in valid_loader:\n",
    "            val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            output = model(inputs, val_h)\n",
    "            val_loss = criterion(output, labels.long())\n",
    "\n",
    "            val_losses.append(val_loss.item())\n",
    "            \n",
    "            accuracy = acc(output,labels)\n",
    "            val_acc += accuracy\n",
    "            \n",
    "    epoch_train_loss = np.mean(train_losses)\n",
    "    epoch_val_loss = np.mean(val_losses)\n",
    "    epoch_train_acc = train_acc/len(train_loader.dataset)\n",
    "    epoch_val_acc = val_acc/len(valid_loader.dataset)\n",
    "    epoch_tr_loss.append(epoch_train_loss)\n",
    "    epoch_vl_loss.append(epoch_val_loss)\n",
    "    epoch_tr_acc.append(epoch_train_acc)\n",
    "    epoch_vl_acc.append(epoch_val_acc)\n",
    "    print(f'Epoch {epoch+1}') \n",
    "    print(f'train_loss : {epoch_train_loss} val_loss : {epoch_val_loss}')\n",
    "    print(f'train_accuracy : {epoch_train_acc*100} val_accuracy : {epoch_val_acc*100}')\n",
    "    if epoch_val_loss <= valid_loss_min:\n",
    "        torch.save(model.state_dict(), './working/state_dict.pt')\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,epoch_val_loss))\n",
    "        valid_loss_min = epoch_val_loss\n",
    "    print(25*'==')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5855837a-929c-4d18-a306-456ada8a7c46",
   "metadata": {},
   "source": [
    "Testing Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f31f0566-4018-44bc-85c4-fc757d953ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.411\n",
      "Test accuracy: 87.293\n"
     ]
    }
   ],
   "source": [
    "# Get test data loss and accuracy\n",
    "\n",
    "test_losses = [] # track loss\n",
    "# num_correct = 0\n",
    "\n",
    "test_acc = 0.0\n",
    "\n",
    "# init hidden state\n",
    "test_h = model.init_hidden(batch_size)\n",
    "\n",
    "model.eval()\n",
    "# iterate over test data\n",
    "for inputs, labels in test_loader:\n",
    "\n",
    "    # Creating new variables for the hidden state, otherwise\n",
    "    # we'd backprop through the entire training history\n",
    "    test_h = tuple([each.data for each in test_h])\n",
    "\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    \n",
    "    output = model(inputs, test_h)\n",
    "    \n",
    "    # calculate loss\n",
    "    test_loss = criterion(output, labels.long())\n",
    "    test_losses.append(test_loss.item())\n",
    "    \n",
    "    accuracy = acc(output,labels)\n",
    "    test_acc += accuracy\n",
    "\n",
    "\n",
    "# -- stats! -- ##\n",
    "# avg test loss\n",
    "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
    "\n",
    "# accuracy over all test data\n",
    "test_acc = test_acc/len(test_loader.dataset)\n",
    "print(\"Test accuracy: {:.3f}\".format(test_acc*100))\n",
    "\n",
    "# epoch_test_acc = test_acc/len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06cece7-95c0-4814-8e04-0eba4aac6974",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
